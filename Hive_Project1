create table agent_Loging
(
SL_No int,
Date_ string,
Agent string,
Login_Time string,
Logout_Time string,
Duration string
)
row format delimited
fields terminated by ',';



Create table agent_performance
(
SL_No int,
Date_ string,
Agent_Name string,
Total_Chats int,
Average_Response_Time string,
Average_Resolution_Time string,
Average_Rating decimal(5,2),
Total_Feedback int
)
row format delimited
fields terminated by ',';

load data inpath '/tmp/AgentLogingReport.csv' into table agent_Log;
load data inpath '/tmp/AgentPerformance.csv' into table agent_performance;

1. Create a schema based on the given dataset
create hive_project;

2. Dump the data inside the hdfs in the given schema location.
hadoop fs -put AgentPerformance.csv /tmp
hadoop fs -put AgentLogingReports.csv /tmp

3. List of all agents' names. 
Select Distinct Agent_name from agent_performance 
union
Select Distinct Agent from agent_Log;

4. Find out agent average rating.
5. Total working days for each agents 
6. Total query that each agent have taken 
7. Total Feedback that each agent have received 
8. Agent name who have average rating between 3.5 to 4 
9. Agent name who have rating less than 3.5 
10. Agent name who have rating more than 4.5 
11. How many feedback agents have received more than 4.5 average
12. average weekly response time for each agent 
13. average weekly resolution time for each agents 
14. Find the number of chat on which they have received a feedback 
15. Total contribution hour for each and every agents weekly basis 
16. Perform inner join, left join and right join based on the agent column and after joining the table export that data into your local system.
17. Perform partitioning on top of the agent column and then on top of that perform bucketing for each partitioning.
